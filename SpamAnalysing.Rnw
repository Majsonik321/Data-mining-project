\documentclass[12pt, a4paper]{article}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[OT4]{polski}
\usepackage[utf8]{inputenc}
\usepackage[top=2.5cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage[colorlinks=true, linkcolor=blue]{hyperref}
\usepackage{enumerate}
\usepackage{bbm}
\usepackage{polski}
\usepackage{mathtools,amsthm,amssymb}
\usepackage{amsmath}
\mathtoolsset{mathic}
\newcommand{\RomanNumeralCaps}[1]{\MakeUppercase{\romannumeral #1}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
<<ustawienia_globalne,echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE>>=
library(ggplot2)
library(dlookr)
library(kableExtra)
library(tidyverse)
library(dplyr)
library(Epi)
library(kernlab)
library(knitr)
library(xtable)
library(cowplot)
library(corrplot)
library(MASS)
library(caret)
library(pROC)
require(gridExtra)
library(ipred)
library(class)

library(mlbench) 
library(factoextra)
library(gplots)  # heatmap.2
library(stats)   # kmeans
library(cluster) # pam, clara
library(ppclust)
library(factoextra)

#library("fpc") 
library("dbscan")

library(flexclust)

library("FactoMineR")

opts_chunk$set(fig.path='figure/', fig.align='center', fig.pos='H',fig.width=7, fig.height=5)
@

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Formatowanie spamu \\ Data Mining}
\author{Michał Maj i Anna Mieszkalska \\album 256556 i 255699}
\maketitle
\tableofcontents

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Analiza opisowa i wizualizacja} \label{Analiza opisowa}
\subsection{Wstęp}
W naszym projekcie będziemy analizować dane o nazwie \textit{Spambase} z biblioteki \textit{kernlab}. Zestaw danych \textit{spambase} jest zbiorem wiadomości e-mail, które zostały przeanalizowane i sklasyfikwane jako spam lub non-spam. Celem tego zbioru danych jest dostarczenie użytecznych materiałów potrzebnych do analiz i eksploracji w tym zakresie. W tym projekcie użyjemy różnych metod i technik pozyskiwania wiedzy, aby przeanalizować dane \textit{spambase} w celu opracowania modelu klasyfikującego wiadomości e-mail jako spam lub non-spam. Modele opracowane w tym projekcie mogą być przydatne w rzeczywistych serwisach poczt e-mailowych, gdzie problem dostarczania niechcianych wiadomości jest nam powszechnie znany.
\subsection{Opis danych}
Zbiór danych \textit{spambase} wyodrębnia 58 cech, które oznaczają częstość występowania danego znaku bądź słowa w jednym e-mailu. Pierwsze 48 zmiennych dotyczy występowania konkretnych słów, następne 6 występowania znaków, a kolumny 55-57 dotyczą średniej, najdłuższej i całkowitej długości wielkich liter. Ostatnia zmienna \textit{type} odpowiada za określenie typu e-maila jako spam lub non-spam, zatem będziemy rozważać dwie klasy. Zbiór ten składa się z 4601 obserwacji (wiadomości e-mail).
Z powodu dużej obszerności danych przedstawiamy tylko część zmiennych (tabela \ref{tab:part}), następnie pokazujemy rozkład klas wykorzystując różne metody wizualizacji (\ref{tab:rozkład_klas}, \ref{fig:rozkład_klas_plot} i \ref{tab:rozkład_klas_procent}).

<<dane, echo=F, eval=T, results='hide'>>=
#dane
data(spam)

#pierwsze 10 wyników
#head(spam)

#liczba maili
#row(spam)

#View(spam)

#typ zmiennych
#str(spam)
colnames(spam)
#summary(spam)

#sprawdzanie czy są wartości NA
#sum(is.na(spam))

#lub drugi sposób

#any(is.na(spam))

#zmiana nazw kolumn 49-54
spam <- spam %>% 
        rename(",,;''" = "charSemicolon",
               ",,(''" = "charRoundbracket",
               ",,[''" = "charSquarebracket",
               ",,!''" = "charExclamation",
               ",,$''" = "charDollar",
               ",,#''" = "charHash")
@

<<tabela_spam, echo=FALSE, eval=TRUE, results='asis'>>=
#druga przykładowa tabelka
part <- spam[c(1:10,50,58)]
part <- xtable(part, 
               digits = 3, 
               row.names = FALSE, 
               caption = "Spambase - pierwsze 14 rekordów dla wybranych zmiennych.", 
               label = "tab:part")
align(part) <- "c|c|c|c|c|c|c|c|c|c|c|c|c"
print(part[1:14,], type = "latex", table.placement = "H",sanitize.text.function=function(x){x}, scalebox = 0.9)
@

<<rozkład_klas, echo=F, eval=T>>=
#counting spam and non-spam
count_spam <- table(spam$type)
kbl(count_spam, caption = "Rozkład klas.",col.names = c('type','ilość')) %>% kable_styling(bootstrap_options = "striped", full_width = F, position = "center",latex_options = "HOLD_position",font_size = 12)
@

<<rozkład_klas_plot,echo=FALSE, eval=TRUE, fig.pos="H", fig.align="center", out.extra="", fig.height=3, fig.width=6, fig.cap= "Wykres rozkładu klas.">>=
ggplot(spam, aes(x = type, fill = type)) + geom_bar() +
  scale_fill_manual(values = c("#639ADA", "#7C59A4"))
@

<<rozkład_klas_procent, echo=F, eval=T>>=
kbl(round(prop.table(table(spam$type))*100, 2), caption = "Rozkład klas (procentowo).",col.names = c('type','procent')) %>% kable_styling(bootstrap_options = "striped", full_width = F, position = "center",latex_options = "HOLD_position",font_size = 12)
@
\noindent
Ilość e-maili, które zostały sklasyfikowane jako spam wynosi 1813 (tabela \ref{tab:rozkład_klas}), a ilość tych, które nie są spamem wynosi 2733, zatem klasa spam stanowi prawie $40\%$ całości (tabela \ref{tab:rozkład_klas_procent}), więc dane są dość zbalansowane co potwierdza rysunek \ref{fig:rozkład_klas_plot}.
\noindent
Za pomocą funkcji \texttt{str} mamy pokazane, że wszystkie zmienne są typu \textit{numeric}, oczywiście oprócz zmiennej \textit{type}, która jest typu \textit{factor}. Możemy również sprawdzić typ zmiennych za pomocą funkcji \texttt{sapply} i zliczyć ich ilość. Otrzymamy wynik 57 bez zmiennej \textit{type}. 

<<typ_zmiennych, echo=T, eval=T>>=
sum(sapply(spam,is.numeric))
@
\noindent
Patrząc do tabeli \ref{tab:part} widzimy, że wszystkie typy zmiennych zostały określone prawidłowo. Funkcja \texttt{is.na()} mówi nam, że nasze dane nie posiadają żadnych wartości NA, należy jednak sprawdzić, czy w tym przypadku nie są one kodowane inaczej. Możemy użyć funkcji \texttt{complete.cases}, żeby usunąć wszelkie brakujące wartości.

<<brakujace_wartosci, echo=T, eval=T>>=
sum(!complete.cases(spam))
@
\noindent
Wynik wyszedł $0$, zatem nasze dane nie mają żadnych brakujących wartości oraz niestandardowego kodowania.
\newpage
\noindent
Podsumowując, mamy:
\begin{itemize}

\item $n = 4601$ (liczba przypadków),
\item $p = 58$ (liczba cech),
\item $K = 2$ (licza klas, ,,spam'' i ,,nonspam''),
\item 0 wartości brakujących.

\end{itemize}

\subsection{Przygotowanie danych do analizy}
Przed analizą poszczególnych zmiennych należy znormalizować dane, ponieważ są nierównomiernie rozłożone. Zostały one znormalizowane przy użyciu własnoręcznie zaimplementowanej funkcji \texttt{normalize}, tak aby wszystkie wartości liczbowe mieściły się w przedziale od $0$ do $1$.

<<normalizowanie_wartosci, echo=T, eval=T>>=
normalize <- function(x) {
        return( (x - min(x)) / (max(x) - min(x)))
}
NormalizeSpam <- as.data.frame(lapply(spam[1:57], normalize))

#Cleaning Data - dodanie kolumny type (naszej klasy)
CleanSpam <- cbind(spam[, 58], NormalizeSpam)
names(CleanSpam)[names(CleanSpam) == "spam[, 58]"] <- "class"
CleanSpam$class <- as.character(CleanSpam$class)

#usytuowanie zmiennej class na sam koniec
CleanSpam <- CleanSpam %>% relocate(class, .after=capitalTotal)
@

<<zamiana_nazw_kolumn, echo=F, eval=T>>=
#zmiana nazw kolumn 49-54
CleanSpam <- CleanSpam %>% 
        rename(",,;''" = "X.....",
               ",,(''" = "X......1",
               ",,[''" = "X......2",
               ",,!''" = "X......3",
               ",,$''" = "X......4",
               ",,#''" = "X......5")
@
\noindent
W poprzedniej sekcji pokazaliśmy, że nie ma żadnych brakujących wartości. Musimy również usunąć obserwacje zduplikowane, za pomocą funkcji \texttt{distinct} z pakietu \textit{dplyr}. 

<<zduplikowane_wartosci, echo=F, eval=T>>=
CleanSpam <- CleanSpam %>% distinct()
cat("Liczba usuniętych wierszy:",nrow(spam)-nrow(CleanSpam)) #różnica w ilości rekordów po usunięciu duplikatów
@
\noindent
Ilość obserwacji zmniejszyła się i wynosi aktualnie \Sexpr{nrow(CleanSpam)}.
\\
\noindent
Obliczając jeden ze wskaźników sumarycznych otrzymujemy, że większość zmiennych cechuję się bardzo niską wariancją (tabela \ref{tab:var}), co świadczy o bardzo małej zmienności, zatem ciężko będzie nam wybrać zmienne, które powinny zostać usunięte (biorąc pod uwagę tylko ten aspekt).


<<wariancja,echo=FALSE, eval=TRUE,results='asis'>>=
var_spam <- CleanSpam[c(1:10,50)]
var_matrix <- var(var_spam)  
var_matrix <- xtable(var_matrix, 
               digits = 5, 
               row.names = FALSE, 
               caption = "Wariancja poszczególnych zmiennych.", 
               label = "tab:var")
align(var_matrix) <- "c|c|c|c|c|c|c|c|c|c|c|c"
print(var_matrix, type = "latex", table.placement = "H",sanitize.text.function=function(x){x}, scalebox = 0.75)
@
\newpage
\subsection{Analiza poszczególnych zmiennych}
W tej sekcji zajmiemy się pozostałymi wskaźnikami sumarycznymi czyli między innymi miarami położenia i rozrzutu, wyznaczonymi dla wszystkich danych.
\\
\noindent
Wiemy, że wszystkie zmienne oprócz \textit{type} są ilościowe zatem możemy użyć funkcji \texttt{describe} z pakietu \textit{dlookr}, żeby zobaczyć statystyki opisowe.
<<wskazniki_sumaryczne,echo=FALSE, eval=TRUE,results='asis'>>=
kbl(describe(CleanSpam)[c(1:9,15,18,21)], caption = "Wskaźniki sumaryczne dla wszystkich zmiennych.",digits = 4) %>% kable_styling(bootstrap_options = "striped", full_width = F, position = "center",latex_options = "HOLD_position",font_size = 7.6)
@
\noindent
W naszych danych występuje $4210$ obserwacji oraz $0$ wartości brakujących (tabela \ref{tab:wskazniki_sumaryczne}). Jeżeli chodzi o średnią, odchylenie standardowe, błąd standardowy średniej czy rozstęp międzykwartylowy (IQR) to osiągane są dość niskie wyniki dla każdej zmiennej. Podobnie możemy powiedzieć o wartościach osiąganych przez kwartyl pierwszy, drugi i trzeci. Inaczej to się ma jeśli chodzi o skośność gdzie osiągane są wysokie wyniki powyżej zera, co świadczy o prawostronnej asymetrii rozkładu zmiennych. Jeżeli chodzi o wartości kurtozy to są one największe spośród wszystkich wskaźników sumarycznych wymienionych w tabeli \ref{tab:wskazniki_sumaryczne} i mówią o tym, że rozkład zmiennych jest bardziej wysmukły niż normalny (rozkład leptokurtyczny), czyli mają większe skupienie wartości wokół średniej.

\subsection{Analiza zależności (korelacji) między zmiennymi}
W tej sekcji zajmiemy się rysowaniem podstawowych wykresów takich jak histogramy, wykresy rozrzutu, wykresy pudełkowe, macierze korelacji czy estymatory jądrowe gęstości.
\\
\noindent
Na początek tworzymy macierz korelacji, żeby zobaczyć jak prezentują się zależności między zmiennymi.

<<macierz_korelacji,echo=FALSE, eval=TRUE, fig.pos="H", fig.align="center", out.extra="", fig.height=4, fig.width=6, fig.cap= "Macierz korelacji zmiennych.">>=
corrplot(cor(CleanSpam[1:57]), tl.cex = 0.4)
@
\noindent
Zmienne w znacznej większości nie są ze sobą powiązane (tabela \ref{fig:macierz_korelacji}), jednak wyodrębnimy te, dla których zależność jest wysoka. Wybierzemy wartości korelacji zmiennych, dla których wartość bezwzględna współczynnika korelacji jest wyższa niż $0.55$. Wtedy otrzymamy lepszy i wyrazistszy wgląd na zmienne oraz będziemy mogli przejść do dalszej analizy.
<<macierz_korelacji2,echo=FALSE, eval=TRUE, fig.pos="H", fig.align="center", out.extra="", fig.height=3, fig.width=6, fig.cap= "Macierz korelacji poszczególnych zmiennych.">>=
cor_matrix <- cor(CleanSpam[1:57])                     

#modyfikujemy
cor_matrix_rm <- cor_matrix  
diag(cor_matrix_rm) <- 0

#usuwamy kolumny poniżej wartości współczynnika korelacji 0.55
cor_matrix_rm_new <- CleanSpam[ , apply(cor_matrix_rm,2,function(x) any(abs(x) > 0.55))]
names <- names(which(apply(cor_matrix_rm,2,function(x) any(abs(x) > 0.55))))
corrplot(cor(CleanSpam[names]))
@
\noindent
Dla lepszego zobrazowania na rysunku \ref{fig:macierz_korelacji3} przedstawione są dokładne wartości współczynnika korelacji, które są osiągane na rysunku \ref{fig:macierz_korelacji2}.

<<macierz_korelacji3,echo=FALSE, eval=TRUE, fig.pos="H", fig.align="center", out.extra="", fig.height=3.5, fig.width=6, fig.cap= "Macierz korelacji poszczególnych zmiennych z widocznymi wartościami współczynnika korelacji.">>=
corrplot(cor(CleanSpam[names]), method="number",number.cex=0.8)
@
\noindent
Największe powiązanie występuje między zmiennymi \textit{num857} i \textit{num415} gdzie współczynnik korelacji wynosi prawie 1. Może to być związane z numerem telefonu, który składa się z takich cyfr, wtedy numery te tworzyłyby całość co jest równoznaczne z tym, że będą występowały razem w danym mailu lub nie. Zauważalna jest również jedynie korelacja dodatnia oznaczająca, że wraz ze wzrostem wartości jednej cechy następuje wzrost wartości drugiej. 
\newpage
\noindent
Zobaczmy jak zmienne prezentują się na skategoryzowanych wykresach rozrzutu.
<<wykresy_rozrzutu,echo=FALSE, eval=TRUE, fig.pos="H", fig.align="center", out.extra="", fig.height=11, fig.width=10, fig.cap= "Skategoryzowane wykresy rozrzutu poszczególnych zmiennych.">>=
r1 <- ggplot(CleanSpam, aes(x = labs, y = num857, color = class)) + geom_point()

r2 <- ggplot(CleanSpam, aes(x = telnet, y = num857, color = class)) + geom_point()

r3 <- ggplot(CleanSpam, aes(x = num857, y = num415, color = class)) + geom_point()

r4 <- ggplot(CleanSpam, aes(x = num415, y = num857, color = class)) + geom_point()

r5 <- ggplot(CleanSpam, aes(x = technology, y = num857, color = class)) + geom_point()

r6 <- ggplot(CleanSpam, aes(x = direct, y = num857, color = class)) + geom_point()

grid.arrange(r1, r2, r3, r4, r5, r6, ncol = 2, nrow = 3)

@
\noindent
Widzimy na wykresach \ref{fig:wykresy_rozrzutu}, że wraz ze wzrostem jednej obserwacji równomiernie rośnie druga obserwacja, co świadczy o dużej zależności, ale punkty też układają się pionowo, kiedy wartości obserwacji na osi $x$ osiągają $0$ lub poziomo, kiedy wartości obserwacji na osi $y$ osiągają $0$. 
\newpage
\noindent
Następnie tworzymy histogramy wraz z estymatorami jądrowych gęstości dla zmiennych z najwyższym współczynnikiem korelacji, gdzie zmienną grupującą jest cecha \textit{class}.
<<histogramy,echo=FALSE, eval=TRUE, fig.pos="H", fig.align="center", out.extra="", fig.height=16, fig.width=14, fig.cap= "Histogramy dla poszczególnych zmiennych.">>=
h1 <- ggplot(CleanSpam, aes(x=labs, fill=class)) +
  geom_histogram( color='#e9ecef', alpha=0.6, position='identity',bins = 30) + theme(text = element_text(size = 20))

h2 <- ggplot(CleanSpam, aes(x=telnet, fill=class)) +
  geom_histogram( color='#e9ecef', alpha=0.6, position='identity',bins = 30) + theme(text = element_text(size = 20))

h3 <- ggplot(CleanSpam, aes(x=num857, fill=class)) +
  geom_histogram( color='#e9ecef', alpha=0.6, position='identity',bins = 30) + theme(text = element_text(size = 20))

h4 <- ggplot(CleanSpam, aes(x=num415, fill=class)) +
  geom_histogram( color='#e9ecef', alpha=0.6, position='identity',bins = 30) + theme(text = element_text(size = 20))

h5 <- ggplot(CleanSpam, aes(x=technology, fill=class)) +
  geom_histogram( color='#e9ecef', alpha=0.6, position='identity',bins = 30) + theme(text = element_text(size = 20))

h6 <- ggplot(CleanSpam, aes(x=direct, fill=class)) +
  geom_histogram( color='#e9ecef', alpha=0.6, position='identity',bins = 30) + theme(text = element_text(size = 20))

grid.arrange(h1, h2, h3, h4, h5, h6, ncol = 2, nrow = 3)
@
\noindent
Widzimy, na histogramach \ref{fig:histogramy}, że głównie osiągane są wartości równe $0$ oraz większą liczbę stanowi klasa \textit{nonspam}. 
\newpage
\noindent
Przechodzimy do tworzenia estymatorów jądrowych gęstości.
<<estym_jadrowe_gestosci,echo=FALSE, eval=TRUE, fig.pos="H", fig.align="center", out.extra="", fig.height=16, fig.width=14, fig.cap= "Estymatory jądrowe gęstości dla poszczególnych zmiennych.">>=
e1 <- ggplot(CleanSpam, aes(x = labs, fill = class)) +
  geom_density(alpha = 0.7) + theme(text = element_text(size = 20))

e2 <- ggplot(CleanSpam, aes(x = telnet, fill = class)) +
  geom_density(alpha = 0.7) + theme(text = element_text(size = 20))

e3 <- ggplot(CleanSpam, aes(x = num857, fill = class)) +
  geom_density(alpha = 0.7) + theme(text = element_text(size = 20))

e4 <- ggplot(CleanSpam, aes(x = num415, fill = class)) +
  geom_density(alpha = 0.7) + theme(text = element_text(size = 20))

e5 <- ggplot(CleanSpam, aes(x = technology, fill = class)) +
  geom_density(alpha = 0.7) + theme(text = element_text(size = 20))

e6 <- ggplot(CleanSpam, aes(x = direct, fill = class)) +
  geom_density(alpha = 0.7) + theme(text = element_text(size = 20))

grid.arrange(e1, e2, e3, e4, e5, e6, ncol = 2, nrow = 3)
@
\noindent
Wnioski podobne jak w przypadku histogramów, dlatego też wyniki na wykresach \ref{fig:histogramy}, \ref{fig:estym_jadrowe_gestosci} nie są zróżnicowane (kolejny raz wartości 0 stanowią większość).
\newpage
\noindent
Stworzymy tabelę obrazującą jak często pojawia się wartość $0$ dla każdej zmiennej w zbiorze danych \texttt{CleanSpam}.

<<wartosci_zero,echo=FALSE, eval=TRUE,results='asis'>>=
kbl(colSums(CleanSpam==0)[1:57], caption = "Ilość występowania wartości 0 dla każdej zmiennej.",digits = 4,col.names = c('0')) %>% kable_styling(bootstrap_options = "striped", full_width = F, position = "center",latex_options = "HOLD_position",font_size = 7.6)
@
\noindent
Najmniej razy wartość $0$ występuje dla zmiennych \textit{you} $=1146$, \textit{capitalAve} $=216$, \textit{capitalLong} $=216$ i \textit{capitalTotal} $=7$ (tabela \ref{tab:wartosci_zero}). Wykonamy dla tych zmiennych histogramy wraz z dopasowaną gęstością normalną. 

<<histogramy_gestosc,echo=FALSE, eval=TRUE,warning=FALSE, fig.pos="H", fig.align="center", out.extra="", fig.height=16, fig.width=14, fig.cap= "Histogramy wraz z dopasowaną gęstością normalną.">>=
he1 <- ggplot(CleanSpam, aes(x=you)) +  geom_histogram(aes(y=..density..), bins=60, color="black", fill="blue") + 
  stat_function(fun=dnorm, args=list(mean=mean(CleanSpam$you), sd=sd(CleanSpam$you)), col="red", lwd=2) + theme(text = element_text(size = 20))

he2 <- ggplot(CleanSpam, aes(x=capitalAve)) +  geom_histogram(aes(y=..density..), bins=60, color="black", fill="blue") + 
  stat_function(fun=dnorm, args=list(mean=mean(CleanSpam$capitalAve), sd=sd(CleanSpam$capitalAve)), col="red", lwd=2) + theme(text = element_text(size = 20))

he3 <- ggplot(CleanSpam, aes(x=capitalLong)) +  geom_histogram(aes(y=..density..), bins=60, color="black", fill="blue") + 
  stat_function(fun=dnorm, args=list(mean=mean(CleanSpam$capitalLong), sd=sd(CleanSpam$capitalLong)), col="red", lwd=2) + theme(text = element_text(size = 20))

he4 <- ggplot(CleanSpam, aes(x=capitalTotal)) +  geom_histogram(aes(y=..density..), bins=60, color="black", fill="blue") + 
  stat_function(fun=dnorm, args=list(mean=mean(CleanSpam$capitalTotal), sd=sd(CleanSpam$capitalTotal)), col="red", lwd=2) + theme(text = element_text(size = 20))

grid.arrange(he1, he2, he3, he4, ncol = 2, nrow = 2)
@
\noindent
Widzimy, że wartości wciąż są bliskie zeru i są prawoskośnie asymetryczne. Zobaczymy jeszcze czy występują jakiekolwiek wartości odstające stosując wykresy pudełkowe dla zmiennych z najmniejszą liczbą zer, między innymi \textit{will}, \textit{,,(''}, \textit{your}, \textit{you}, \textit{all} i \textit{our},.

<<wykresy_pudelkowe,echo=FALSE, eval=TRUE, fig.pos="H", fig.align="center", out.extra="", fig.height=16, fig.width=14, fig.cap= "Wykresy pudełkowe dla poszczególnych zmiennych.">>=
p1 <- ggplot(CleanSpam, aes(x = class, fill=class, y=will)) + geom_boxplot() + theme(text = element_text(size = 20))

p2 <- ggplot(CleanSpam, aes(x = class, fill=class, y=your)) + geom_boxplot() + theme(text = element_text(size = 20))

p3 <- ggplot(CleanSpam, aes(x = class, fill=class, y=`,,(''`)) + geom_boxplot() + theme(text = element_text(size = 20))

p4 <- ggplot(CleanSpam, aes(x = class, fill=class, y=you)) + geom_boxplot() + theme(text = element_text(size = 20))

p5 <- ggplot(CleanSpam, aes(x = class, fill=class, y=all)) +geom_boxplot() + theme(text = element_text(size = 20))

p6 <- ggplot(CleanSpam, aes(x = class, fill=class, y=our)) + geom_boxplot() + theme(text = element_text(size = 20))

grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 2, nrow = 3)
@
\noindent
Na rysunku \ref{fig:wykresy_pudelkowe} widać, że istnieje dużo wartości odstających i występują one głównie dla klasy \textit{nonspam}. Tylko dla powyższych zmiennych wykresy pudełkowe są ,,widoczne'', a dla pozostałych kontury pudełek wysmuklają się (mniej więcej tak jak w przypadku wykresu pudełkowego dla zmiennej ,,('', tylko oczywiście w mniejszym stopniu) na poziomie równym 0 i nie są widoczne.
\newpage
\noindent
Na sam koniec przejdziemy do usunięcia z naszych danych zmiennych które są ze sobą najbardziej skorelowane, czyli \textit{labs, telnet, num857, num415, technology} i \textit{direct}.

<<usuniecie_zmiennych,echo=TRUE, eval=TRUE>>=
CleanSpam <- CleanSpam[,-c(30:32,34,36,40)]
colnames(CleanSpam)
@
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Klasyfikacja}
Klasyfikacja to proces przypisywania obiektów do jednej z klas na podstawie ich cech. W naszym przypadku klasyfikacja będzie polegać na przewidywaniu, czy dany e-mail jest spamem czy nie. Do tego celu wykorzystamy parę metod klasyfikacji takich jak regresja liniowa, analiza dyskryminacyjna czy algorytm kNN. Naszym celem będzie wybranie metody, która radzi sobie najlepiej z ocenianiem prawdopodobieństwa przynależności do danej klasy. Aby to ocenić użyjemy różnych miar jakości takich jak dokładność, czy krzywa ROC. Pierwszym krokiem będzie podział danych na zbiór uczący i testowy w proporcji 70:30, ponieważ będzie nam to potrzebne do metod klasyfikacji.
<<podzial_test_train, echo=TRUE, eval=TRUE>>=
set.seed(123456)
classify_data <- CleanSpam

train_indices <- sample(nrow(classify_data), 0.7 * nrow(classify_data))
train_data <- classify_data[train_indices, ]
test_data <- classify_data[-train_indices, ]
@

\subsection{Klasyfikacja oparta na regresji liniowej}

\begin{equation}
Y = r(x) + \varepsilon = \beta_0 + \sum_{i=1}^p \beta_i x_i + \varepsilon,
\label{eq:regression}
\end{equation}
W naszym przypadku użyjemy modelu regresji liniowej \ref{eq:regression}, aby wyznaczyć klasyfikator postaci

\begin{equation*}
\hat{G} = \left\{
\begin{aligned}
& \text{SPAM}\quad \text{jeżeli} \quad \hat{Y} > 0.5, \\
& \text{NONSPAM}\quad \text{jeżeli} \quad \hat{Y} \leqslant 0.5,
\end{aligned}
\right.
\end{equation*}

\noindent gdzie $\hat{Y} = r(X) = \hat{\beta_0} + \sum_{j=1}^p X_j \hat{\beta_j}$ oznacza prognozę zmiennej zależnej $Y$, która jest wektorem zawierającym etykietki klas. Dla uproszczenia obliczeń użyjemy wbudowanej funkcji \texttt{lm()} z pakietu \texttt{stats}. W tym celu zmienimy etykietki naszych klas na 0 oraz 1, ponieważ mamy do czynienia z klasyfikacją binarną ($K = 2$).
<<klasyfikacja_regresja_lm1, echo=F, eval=TRUE>>=
# Utworzenie modelu klasyfikacji opartej na regresji liniowej
train_data_lm <- train_data
test_data_lm <- test_data
@

<<klasyfikacja_regresja_lm, echo=TRUE, eval=TRUE>>=
train_data_lm$class <- ifelse(train_data$class == "spam", 1, 0)
test_data_lm$class <- ifelse(test_data$class == "spam", 1, 0)
model <- lm(class ~ ., data = train_data_lm)
predictions <- predict(model, newdata = test_data_lm)
threshold <- 0.5
predicted_labels <- ifelse(predictions > threshold, 1, 0)
@

<<klasyfikacja_regresja_lm_miary, echo=F, eval=TRUE>>=
actual_labels <- test_data_lm$class
tabela_lm <- table(predicted_labels, actual_labels)
rownames(tabela_lm) <- c("nonspam", "spam")
colnames(tabela_lm) <- c("nonspam", "spam")
kbl(tabela_lm , caption = "Macierz kotyngencji dla regresji liniowej.",digits = 4) %>% kable_styling(bootstrap_options = "striped", full_width = F, position = "center",latex_options = "HOLD_position",font_size = 10)

# Obliczenie dokładności klasyfikacji
accuracy1 <- sum(predicted_labels == actual_labels) / length(actual_labels)
cat("Accuracy:", accuracy1)
@

\subsection{Klasyfikacja oparta na metodzie LDA (liniowa analiza dyskryminacyjna)}

Do przeprowadzenia liniowej analizy dyskryminacyjnej użyjemy funkcji \texttt{lda()} z pakietu \texttt{MASS}. Biorąc pod uwagę model składający się ze wszystkich zmiennych oczekujemy, że dostaniemy równoważny wynik jak dla regresji liniowej, ponieważ mamy klasyfikację binarną.

<<klasyfikacja_lda, echo=TRUE, eval=TRUE>>=
lda.model <- lda(class ~ ., data = classify_data, subset=train_indices)
lda.pred <- predict(lda.model, newdata = test_data)
@


<<klasyfikacja_lda_miary, echo=F, eval=TRUE>>=
class_label_lda <- lda.pred$class
actual <- classify_data$class[-train_indices] # rzeczywiste etykietki dla obiektów ze zbioru testowego
conf.mat.lda4 <- table(class_label_lda, actual) # macierz kontyngencji (Confusion matrix)

kbl(conf.mat.lda4, caption = "Macierz kontyngencji dla metody lda.",digits = 4) %>% kable_styling(bootstrap_options = "striped", full_width = F, position = "center",latex_options = "HOLD_position",font_size = 10)

accuracy2 <- sum(diag(conf.mat.lda4)) / sum(conf.mat.lda4)
cat("Accuracy:", accuracy2)
@
\noindent
Z uwagi na to, że rozkład klas nie jest idealnie równomierny (tabela \ref{tab:rozkład_klas_procent}) możemy narysować wykres krzywej ROC umożliwiający ocenę jakości klasyfikacji.
<<klasyfikacja_lda_miary_roc, echo=F, eval=TRUE, warning = F,message=FALSE,fig.pos="H", fig.align="center", out.extra="", fig.height=3, fig.width=6, fig.cap= "Wykres krzywej ROC dla metody lda">>=
roc_obj <- roc(response = test_data$class, predictor = lda.pred$posterior[,2])

# Tworzymy wykres ROC
ggroc(roc_obj, legacy.axes = TRUE) + 
  labs(title = "Receiver Operating Characteristic Curve",
       x = "False Positive Rate",
       y = "True Positive Rate")

@
\noindent
Zastosowana klasyfikacja z użyciem metody lda radzi sobie dość dobrze, ponieważ powierzchnia pod krzywą ROC \ref{fig:klasyfikacja_lda_miary_roc} zajmuje duży obszar, co świadczy o wysokiej jakości klasyfikacji.

\subsection{Klasyfikacja z wykorzystaniem algorytmu kNN (Classification And Regression Training)}
Na początek zajmiemy się klasyfikacją z wykorzystaniem algorytmu k najbliższych sąsiadów za pomocą funkcji \texttt{knn} wbudowanej w bibliotece \textit{class}. W klasyfikacji wykorzystamy parametr liczby sąsiadów $k = 1$.
<<klasyfikacja_kNN_1, echo=TRUE, eval=TRUE>>=
#etykiety klas
TrainLabels <- train_data[, 52]
TestLabels <- test_data[, 52]

# rzeczywiste klasy spamu
etykietki.rzecz <- test_data$class

#prognoza
etykietki.prog <- knn(train = train_data[, 1:51], 
                   test = test_data[, 1:51],
                   cl = TrainLabels, k = 1)
@

<<klasyfikacja_kNN_miary, echo=F, eval=TRUE>>=
# tablica kontyngencji
wynik.tablica <- table(etykietki.prog,etykietki.rzecz)
kbl(wynik.tablica, caption = "Macierz kontyngencji dla metody knn.",digits = 4) %>% kable_styling(bootstrap_options = "striped", full_width = F, position = "center",latex_options = "HOLD_position",font_size = 10)

accuracy3 <- sum(diag(wynik.tablica)) / sum(wynik.tablica)
cat("Accuracy:", accuracy3)
@
\noindent
Widzimy w macierzy kontyngencji \ref{tab:klasyfikacja_kNN_miary}, że zmienne zostały przyzwoicie sklasyfikowane o czym świadczy wysoka, prawie $90$\% dokładność. W tabeli \ref{tab:klasyfikacja_kNN_rozne_k} zobaczymy jak przedstawiają się wartości prawdziwie negatywne, pozytywne oraz dokładność jeżeli zwiększymy wartości parametru $k$.

<<klasyfikacja_kNN_rozne_k, echo=F, eval=TRUE>>=
# etykietki dla różnych wartości k
etykietki.prog.k5 <- knn(train = train_data[, 1:51], 
                   test = test_data[, 1:51],
                   cl = TrainLabels, k = 5)
etykietki.prog.k10 <- knn(train = train_data[, 1:51], 
                   test = test_data[, 1:51],
                   cl = TrainLabels, k = 10)
etykietki.prog.k15 <- knn(train = train_data[, 1:51], 
                   test = test_data[, 1:51],
                   cl = TrainLabels, k = 15)
etykietki.prog.k20 <- knn(train = train_data[, 1:51], 
                   test = test_data[, 1:51],
                   cl = TrainLabels, k = 20)
etykietki.prog.k25 <- knn(train = train_data[, 1:51], 
                   test = test_data[, 1:51],
                   cl = TrainLabels, k = 25)

#tablice kontyngencji
wynik.tablica.k5 <- table(etykietki.prog.k5,etykietki.rzecz)
wynik.tablica.k10 <- table(etykietki.prog.k10,etykietki.rzecz)
wynik.tablica.k15 <- table(etykietki.prog.k15,etykietki.rzecz)
wynik.tablica.k20 <- table(etykietki.prog.k20,etykietki.rzecz)
wynik.tablica.k25 <- table(etykietki.prog.k25,etykietki.rzecz)

#dokładność
accuracy.k5 <- sum(diag(wynik.tablica.k5)) / sum(wynik.tablica.k5)
accuracy.k10 <- sum(diag(wynik.tablica.k10)) / sum(wynik.tablica.k10)
accuracy.k15 <- sum(diag(wynik.tablica.k15)) / sum(wynik.tablica.k15)
accuracy.k20 <- sum(diag(wynik.tablica.k20)) / sum(wynik.tablica.k20)
accuracy.k25 <- sum(diag(wynik.tablica.k25)) / sum(wynik.tablica.k25)

#tabela
Tabela.kNN <- data.frame(k = c(5,10,15,20,25), 
                         'True Negatives' = c(wynik.tablica.k5[1,1],
                                            wynik.tablica.k10[1,1],
                                            wynik.tablica.k15[1,1],
                                            wynik.tablica.k20[1,1],
                                            wynik.tablica.k25[1,1]),
                         'True Positives' = c(wynik.tablica.k5[2,2],
                                            wynik.tablica.k10[2,2],
                                            wynik.tablica.k15[2,2],
                                            wynik.tablica.k20[2,2],
                                            wynik.tablica.k25[2,2]),
                         Accuracy = c(accuracy.k5,accuracy.k10,
                                      accuracy.k15,accuracy.k20,
                                      accuracy.k25))
colnames(Tabela.kNN) <- c('k','True Negatives','True Positives','Accuracy') 
kbl(Tabela.kNN, caption = "Wartości prawdziwie negatywne i prawdziwie pozytywne oraz dokładność, dla różnych k.",digits = 4) %>% kable_styling(bootstrap_options = "striped", full_width = F, position = "center",latex_options = "HOLD_position",font_size = 10)
@
\noindent
Widzimy, że wartości prawdziwie negatywne początkowo wzrastają, a póżniej się stabilizują, natomiast wartości prawdziwie pozytywne i dokładność maleją. Oznacza to, że im większa wartość parametru liczby sąsiadów $k$ tym mniejsza dokładność klasyfikacji.
\newpage
\noindent
Przechodzimy do klasyfikacji z wykorzystaniem funkcji \texttt{ipredknn} z pakietu \textit{ipred} dla parametru liczby sąsiadów $k = 1$.

<<zmiana nazw kolumn, echo = FALSE, eval = TRUE>>==
#Na potrzeby użycia funkcji ipredknn
train_data <- train_data %>% 
        rename("charSemicolon" = ",,;''",
               "charRoundbracket" = ",,(''",
               "charSquarebracket" = ",,[''",
               "charExclamation" = ",,!''",
               "charDollar" = ",,$''",
               "charHash" = ",,#''")
test_data <- test_data %>% 
        rename("charSemicolon" = ",,;''",
               "charRoundbracket" = ",,(''",
               "charSquarebracket" = ",,[''",
               "charExclamation" = ",,!''",
               "charDollar" = ",,$''",
               "charHash" = ",,#''")
#utworzenie prawidłowych nazw kolumn
colnames(train_data) <- make.names(colnames(train_data))
colnames(test_data) <- make.names(colnames(test_data))
@

<<klasyfikacja_kNN_2, echo = TRUE, eval = TRUE>>==
# budujemy model
model.knn.1 <- ipredknn(class ~ ., data=train_data, k=1)

#jakość modelu
etykietki.prog.ipred <- predict(model.knn.1,test_data,type="class")
@

<<klasyfikacja_kNN_ipred, echo=F, eval=TRUE>>=
# tablica kontyngencji
wynik.tablica.ipred <- table(etykietki.prog.ipred,etykietki.rzecz)
kbl(wynik.tablica.ipred, caption = "Macierz kontyngencji dla metody knn.",digits = 4) %>% kable_styling(bootstrap_options = "striped", full_width = F, position = "center",latex_options = "HOLD_position",font_size = 10)
# dokładność
accuracy4 <- sum(diag(wynik.tablica.ipred)) / sum(wynik.tablica.ipred)
cat("Accuracy:", accuracy4)
@
\noindent
Po macierzy kontyngencji \ref{tab:klasyfikacja_kNN_ipred} można zobaczyć że wyniki są bardzo podobne jak w macierzy \ref{tab:klasyfikacja_kNN_miary}, gdzie wykorzystywaliśmy funkcję \texttt{knn} z pakietu \textit{class}. W przypadku dokładności wyniki również są do siebie zbliżone.

\subsection{Inne (zaawansowane) sposoby oceny jakości klasyfikacji}

Często stosuje sie metodę cross-validation polegającą na wielokrotnym losowaniu zbioru uczącego i testowego, budowie klasyfikatora na zbiorze uczącym, sprawdzenia go na testowym oraz uśrednieniu wyników. Można to zrobić oczywiście używając pętli i uśrednić, ale jest jeszcze prostszy sposób.
\\
\noindent
Można skorzystać z gotowych funkcji ponownie z pakietu \textit{ipred}, ale należy przygotować sobie ,,wrapper'' dostosowujący funkcję \texttt{predict} dla naszego modelu do standardu wymaganego przez \texttt{errorest}.

<<zaawansowane_metody_klasyfikacji, echo = TRUE, eval = TRUE>>==
my.predict  <- function(model, newdata)
  predict(model, newdata=newdata, type="class")
my.ipredknn <- function(formula1, data1, ile.sasiadow)
  ipredknn(formula=formula1,data=data1,k=ile.sasiadow)
@
\noindent
Przejdziemy do porównania błędów klasyfikacji następujących metod oceny dokładności klasyfikacji:
\begin{itemize}
\item CV (Cross-validation)
\item boot (bootstrap)
\item 632plus
\end{itemize}
<<zaawansowane_nazw_spam, echo = FALSE, eval = TRUE>>==
#zmiana nazw kolumn 49-54
spam <- spam %>% 
        rename("charSemicolon" = ",,;''",
               "charRoundbracket" = ",,(''",
               "charSquarebracket" = ",,[''",
               "charExclamation" = ",,!''",
               "charDollar" = ",,$''",
               "charHash" = ",,#''")
colnames(spam) <- make.names(colnames(spam))
@
\newpage
<<zaawansowane_metody_klasyfikacji_errorest, echo = TRUE, eval = TRUE, cache=TRUE>>==
# porownanie błędów klasyfikacji: cv, boot, .632plus
errorest(type ~., spam, model=my.ipredknn, predict=my.predict, estimator="cv",
         est.para=control.errorest(k = 10), ile.sasiadow=5)
errorest(type ~., spam, model=my.ipredknn, predict=my.predict, estimator="boot",
         est.para=control.errorest(nboot = 50), ile.sasiadow=5)
errorest(type ~., spam, model=my.ipredknn, predict=my.predict, estimator="632plus",
         est.para=control.errorest(nboot = 50), ile.sasiadow=5)
@
\newpage
\noindent
W tabeli \ref{tab:dokladnosc_metody_zaawansowane} pokazane są wartości wskaźnika accuracy dla każdej z trzech metod oceny dokładności klasyfikacji.

<<dokladnosc_metody_zaawansowane, echo = FALSE, eval = TRUE>>==
# porownanie błędów klasyfikacji: cv, boot, .632plus
Tabela.oceny.metody <- data.frame('Cross-validation' = 0.806, 
                         'Bootstrap' = 0.781,
                         '632plus' = 0.8094)
colnames(Tabela.oceny.metody) <- c('Cross-validation','Bootstrap','632plus') 
kbl(Tabela.oceny.metody, caption = "Wartości wskaźnika accuracy dla różnych metod oceny dokładności klasyfikacji.",digits = 4) %>% kable_styling(bootstrap_options = "striped", full_width = F, position = "center",latex_options = "HOLD_position",font_size = 10)
@
\noindent
Zważywszy na dość niskie wyniki wartości wskaźnika accuracy, powyższe sposoby oceny dokładności nie będą brane pod uwagę w końcowym porównaniu wszystkich metod klasyfikacji.
\textbf{Podsumowanie}
\\
W tym rozdziale przeprowadzono klasyfikację na zbiorze danych przy użyciu różnych metod, takich jak regresja liniowa, LDA i kNN. Wszystkie metody zostały ocenione na podstawie ich skuteczności, mierzonej za pomocą wskaźnika accuracy. Najwyższe wyniki (tabela \ref{tab:klasyfikacja_podsumowanie}) uzyskano dla metody kNN dla liczby sąsiadów równej 5, osiągając skuteczność na poziomie prawie $90\%$, co wskazuje na jej wysoką zdolność do poprawnej klasyfikacji obiektów. Tak jak podejrzewaliśmy metody LDA oraz regresji liniowej osiągneły prawie taki sam poziom accuracy, który był również wysoki.

<<klasyfikacja_podsumowanie, echo=F, eval=TRUE>>=
#tabela
Tabela.podsumowanie <- data.frame('first' = accuracy1, 
                         'second' = accuracy2,
                         'third' = accuracy3,
                         'fourth' = accuracy.k5,
                         "fifth" = accuracy4)
colnames(Tabela.podsumowanie) <- c('Reg. liniowa','Metoda LDA','Algorytm kNN (k=1)', 'Algorytm kNN (k=5)' ,'Algorytm kNN (funk. ipredknn)') 
kbl(Tabela.podsumowanie, caption = "Wartości wskaźnika accuracy dla różnych metod klasyfikacji.",digits = 4) %>% kable_styling(bootstrap_options = "striped", full_width = F, position = "center",latex_options = "HOLD_position",font_size = 10)
@
\noindent
Wskaźnik accuracy (tabela \ref{tab:klasyfikacja_podsumowanie}) dla wszystkich metod jest do siebie zbliżony, dlatego możemy wnioskować, że wszystkie metody klasyfikacji są w stanie skutecznie ocenić klasę danej obserwacji dla danych \textit{Spambase}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Analiza skupień}
Analiza skupień (ang. cluster analysis) jest techniką statystyczną służącą do grupowania podobnych obiektów w tzw. skupienia (klastry) na podstawie cech lub wzorców, które mają ze sobą pewne podobieństwo. Jest to popularna metoda analizy danych, która pozwala odkrywać struktury i wzorce w zbiorach danych. Głównym celem analizy skupień jest znalezienie naturalnych grup lub klastrów w zbiorze danych, które mają pewne podobieństwo wewnątrz klastra i różnice między klastrami. Podczas przeprowadzania analizy skupień istotne jest dobranie odpowiedniej metody grupowania, takiej jak k-means, PAM, AGNES i inne, w zależności od charakteru danych i celu analizy. Ważne jest również wybranie odpowiednich miar podobieństwa lub odległości między obiektami, które będą brane pod uwagę podczas tworzenia klastrów. Analiza skupień może być użytecznym narzędziem do wydobycia ukrytych informacji i struktur w danych, co pozwala na lepsze zrozumienie badanych zjawisk i podejmowanie bardziej trafnych decyzji.

\subsection{Metody grupujące (ang. partitioning methods)}
Celem metod \textit{grupujących} jest znalezienie podziału obiektów na \textit{K} grup tak, aby optymalizować określone kryterium. W metodach tego typu zadajemy parametr \textit{K}, który oznacza liczbę klastrów oraz \textit{d}$(\cdot , \cdot)$, czyli miarę odmienności.

Do wyboru optymalnej liczby klastrów \texttt{k} możemy użyć metody \textit{elbow}, która jest używana w tym właśnie celu. Na osi X umieszczamy kolejne wartości \texttt{k}, a na osi Y sumę kwadratów odległości między punktami danych, a ich nabliższymi centroidami. Poszukujemy punktu, w którym zmiana sumy kwadratów odległości zaczyna się zmniejszać. Ten punkt wskazuje na optymalną liczbę klastrów.
<<best_k, echo=F, eval=TRUE, fig.width=5, fig.height= 6,fig.pos="H", fig.align="center", out.extra="",message=FALSE,warning=FALSE, fig.cap= "Wizualizacja metody elbow">>=
spam_variables <- CleanSpam[, -52] #removing class labels
spam_variables <- scale(spam_variables) #standardization
actual_labels_spam <- CleanSpam[, 52]

#split into k-clusters
k <- 3
kmeans.k3 <- kmeans(spam_variables,centers=k,iter.max=10, nstart=10) #nstart - liczba inicjalizacji

elbow <- flexclust::bclust(spam_variables, 3, base.k=20)
plot(elbow)

#Trzeba będize się zdecydować na ukazanie jednego wykresu spośród 2 a jeżeli ten dolny to bez tego dendogramu
@
Wykres \textit{elbow} \ref{fig:best_k} wskazuje na wybór \texttt{k} = 3 lub 6, ponieważ wtedy zmiana wartości na osi Y zaczyna "wyhamowywać".

\newpage

\begin{itemize}

\item \textbf{Metoda K-means}

Pierwszą i najbardziej podstawową metodą grupującą, którą omówimy, będzie algorytm \textbf{k-średnich}. Możemy go interpretować jako iteracyjne rozwiązanie zadania minimalizacji kryterium $$W(C) = \sum_{i=1}^{n}||x_{i} - m_{C(i)}||^{2}.$$ Algorytm ten nie może zostać zastosowany dla cech jakościowych oraz cechuje się małą złożonością obliczeniową, dlatego też możemy go użyć dla danych \textit{spam}. Przy wnioskowaniu musimy wziąć pod uwagę, że wynik zależy od początkowego wyboru centrów skupień oraz że metoda jest wrażliwa na obserwacje odstające.

<<anal_skup_kmeans_plot_PCA_k3, echo=F, eval=TRUE, fig.width=5, fig.height= 3,fig.pos="H", fig.align="center", out.extra="", fig.cap= "Wizualizacja na bazie PCA dla metody k-means dla k = 3">>=

#Dajemy tylko wykresy PCA bo mamy za dużo zmiennych, żeby się nie bawić
#Będzie ta metoda stosowana dla innych metod, żeby łatwiej było porównać (chyba?)

#Visualization based on PCA (Principal Component Analysis)
fviz_cluster(kmeans.k3, spam_variables) +
  scale_colour_manual(values = c("darkslategray2", "deepskyblue", "dodgerblue4")) +
  scale_fill_manual(values = c("darkslategray2", "deepskyblue", "dodgerblue4")) 

kmeans.k4 <- kmeans(spam_variables,centers=4,iter.max=10, nstart=10)

kmeans.k6 <- kmeans(spam_variables,centers=6,iter.max=10, nstart=10)
@

<<anal_skup_kmeans_plot_PCA_k4, echo=F, eval=F, fig.width=5, fig.height= 3,fig.pos="H", fig.align="center", out.extra="", fig.cap= "Wizualizacja na bazie PCA dla metody k-means dla k = 4">>=
#CZEMU TO NIE DZIAŁA !?!?!?!!?!?!?!
fviz_cluster(kmeans.k4, spam_variables) +
  scale_colour_manual(values = c("darkslategray2", "deepskyblue", "dodgerblue4")) +
  scale_fill_manual(values = c("darkslategray2", "deepskyblue", "dodgerblue4")) 
@

<<anal_skup_kmeans_plot_PCA_k6, echo=F, eval=F, fig.width=6, fig.height= 4, fig.pos="H", fig.align="center", out.extra="", fig.cap= "Wizualizacja na bazie PCA dla metody k-means dla k = 6">>=
fviz_cluster(kmeans.k6, spam_variables) +
  scale_colour_manual(values = c("darkslategray2", "deepskyblue", "dodgerblue4")) +
  scale_fill_manual(values = c("darkslategray2", "deepskyblue", "dodgerblue4")) 
@

Każda wiadomość e-mail została przypisana do jednej z grup, oznaczonych różnymi kolorami, na podstawie podobieństwa cech. W dużej części wykresu (%\ref{})
obserwujemy wyraźne oddzielenie grup na wykresie, oznacza to, że duża część wiadomości w tych grupach posiada różne charakterystyki i można je w miarę łatwo rozróżnić na podstawie tych cech. Jednakże grupy zauważalnie nakładają się na siebie przy ich "łączeniach", a to mówi nam, że niektóre cechy wiadomości są podobne między grupami i może być je trudniej rozróżnić i prawidłowo przyporządkować.

\item \textbf{Algorytm  PAM}

\textbf{Partition Around Medoids (PAM)} jest jednym z uogólnień metody \textit{k-means}. Polega na poszukiwaniu \textit{K} reprezentatywnych obiektów wybranych ze zbioru wszystkich obiektów tak, aby minimalizować kryterium $$W(C) = \sum^{n}_{i = 1}d(x_{i}, m_{j(i)}),$$ gdzie $m_{j(i)}$ oznacza centrum skupienia najbliższe do obserwacji $x_{i}$ oraz $j = 1,\dots, K$. Algorytm \textit{PAM} może być stosowany dla cech różnego typu, lecz w naszym przypadku nic to nie zmienia, ponieważ dane \textit{spam} składają się tylko z cech numerycznych. Jest on również bardziej odporny na występowanie obserwacji odstających niż metoda \textit{k-means}. Niestety dla dużych zbiorów danych złożoność obliczeniowa dla tej metody jest wysoka, dlatego czas obliczeń dla naszych danych, gdzie liczba obserwacji $n$  wynosi 4601, byłby za długi, dlatego wskazane będzie pominięcie tej metody i przejście do następnej.


\item \textbf{Metoda CLARA}

\textbf{Algorytm Clara} jest rozszerzeniem metody PAM stworzonym właśnie na potrzeby analizy danych zawierających dużą liczbę obiektów, zatem możemy go zastosować, ponieważ charakteryzuje się on zredukowanym czasem obliczeniowym. Taki aspekt jest osiągany dzięki odpowiedniemu próbkowaniu danych. %jakoś inaczej to ostatnie zdanie :(
W algorytmie tym zaczynamy od losowego wyboru podzbiorów danych ustalając liczbę podzbiorów oraz ich wielkość. Następnie dla tak wybranego podzbioru stosujemy algorytm \textit{PAM}, wyznaczając podział na \textit{K} skupień. Następnie przeprowadzamy pogrupowanie całego zbioru danych z wykorzystaniem wyznaczonego zbioru \textit{K} medoidów. Wybieramy liczbę klastrów \texttt{k} $= 3$.

<<anal_skup_CLARA, echo=F, eval=T, fig.width=5, fig.height= 3,fig.pos="H", fig.align="center", out.extra="", fig.cap= "Partycja na bazie algorytmu CLARA">>=

#wszędzie tutaj trzeba ładnie podpisać wykresy, pozmieniać kolorki oraz zmienić ich wielkość, ale to zabawa na potem

# Algorytm CLARA
# KMEANS_3 <- kmeans(spam_variables, 3)
part.CLARA <- clara(spam_variables, k=3, samples=100, sampsize=1000)
# fviz_cluster(list(data=spam_variables, cluster=KMEANS_3$cluster), 
#              ellipse.type="norm", geom="point", stand=FALSE, palette="jco", ggtheme=theme_classic())

fviz_cluster(part.CLARA, main = "CLARA", ellipse = FALSE, geom = "point", xlab="x", ylab="y", show.clust.cent = TRUE, pointsize = .5) +
  scale_colour_manual(values = c("darkslategray2", "deepskyblue", "dodgerblue4")) +
  scale_fill_manual(values = c("darkslategray2", "deepskyblue", "dodgerblue4")) 

@

<<anal_skup_CLARA_field, echo=F, eval=F, fig.width=5, fig.height= 3,fig.pos="H", fig.align="center", out.extra="", fig.cap= "Partycja na bazie algorytmu CLARA z zaznaczonym obszarem dla k = 3, 4 i 6.">>=

#wszędzie tutaj trzeba ładnie podpisać wykresy, pozmieniać kolorki oraz zmienić ich wielkość, ale to zabawa na potem
par(mfrow = c(1,3))
fviz_cluster(part.CLARA, main = "CLARA", ellipse = TRUE, geom = "point", xlab="x", ylab="y", show.clust.cent = TRUE, pointsize = .5) +
  scale_colour_manual(values = c("darkslategray2", "deepskyblue", "dodgerblue4")) +
  scale_fill_manual(values = c("darkslategray2", "deepskyblue", "dodgerblue4")) 

part.CLARA <- clara(spam_variables, k=4, samples=100, sampsize=1000)

fviz_cluster(part.CLARA, main = "CLARA", ellipse = TRUE, geom = "point", xlab="x", ylab="y", show.clust.cent = TRUE, pointsize = .5) +
  scale_colour_manual(values = c("darkslategray2", "deepskyblue", "dodgerblue4")) +
  scale_fill_manual(values = c("darkslategray2", "deepskyblue", "dodgerblue4")) 

part.CLARA <- clara(spam_variables, k=6, samples=100, sampsize=1000)

fviz_cluster(part.CLARA, main = "CLARA", ellipse = TRUE, geom = "point", xlab="x", ylab="y", show.clust.cent = TRUE, pointsize = .5) +
  scale_colour_manual(values = c("darkslategray2", "deepskyblue", "dodgerblue4")) +
  scale_fill_manual(values = c("darkslategray2", "deepskyblue", "dodgerblue4")) 
@
Obszary na powyższych wykresach nachodzą na siebie. %opisać lepiej lub nie

\item \textbf{Fuzzy c-means (FCM)}

W przeciwieństwie do poprzednich metod grupujących, algorytm \textbf{fuzzy c-means} pozwala na przynależność danej obserwacji do kilku skupień równocześnie. Jest to alternatywne podejście nazywane klasteryzacją rozmytą (ang. fuzzy clustering). Idea algorytmu jest podobna do metody \textit{k-means}, a podstawową różnicą jest fakt, że dla każdego punktu $x_{i}$ wyznaczamy zestaw współczynników $\{w_{i,k}\}_{k=1,\dots,K}$ określający jego stopień przynależności do \textit{k}-tego skupienia.

\textit{FCM} możemy interpretować jako iteracyjne rozwiązanie zagadnienia minimalizacji kryterium $$J_{m} = \sum^{n}_{i = 1}\sum^{K}_{k = 1}w^{m}_{i, k}||x_{i} - c_{k}||^{2}, \quad 1 \leq m < \infty,$$ gdzie $$w_{i, k} = \frac{1}{\sum^{K}_{j = 1}\left( \frac{||x_{i} - c_{k}||}{||x_{i} - c{j}||} \right)^{\frac{2}{m-1}}}.$$
\\
\\
Rysujemy wykres \ref{fig:anal_skup_Fuzzy_field} za pomocą funkcji \texttt{fviz\_cluster} dla algorytmu Fuzzy-means
<<anal_skup_Fuzzy_field, echo=F, eval=T, fig.width=5, fig.height= 3,fig.pos="H", fig.align="center", out.extra="", fig.cap= "Partycja na bazie algorytmu FCM z zaznaczonym obszarem",warning=F>>=

#długo liczy coś, nie wiadomo czy działa czy nie

# algorytm FCM 
partycja.fcm <- fcm(spam_variables, centers=3)

res.fcm <- ppclust2(partycja.fcm, "kmeans")
fviz_cluster(res.fcm, data = spam_variables, 
  ellipse.type = "convex",
  palette = "jco",
  repel = TRUE)
@

\end{itemize}

\subsection{Metody hierarchiczne}
Wynik metod \textit{hierarchicznych} przedstawiony jest w postaci hierarchii zagnieżdżonych skupień (tzw. dendrogram). Nie jest wymagane zadanie z góry liczby skupień \textit{K}, możemy zdecydować o tym później po zaznajomieniu się z wynikami. Wśród metod hierarchicznych wyróżniamy dwa typy metod.
\begin{itemize}

\item \textbf{Metody aglomeracyjne} początkowo traktują każdy obiekt jako osobno skupienie. W następnych etapach najbliższe sobie skupienia są łączone, aż do momentu utworzenia jednego dużego skupienia zawierającego wszystkie obiekty.

Jednym z przykładowych algorytmów, który przedstawimy będzie \textbf{metoda AGNES}.

Możemy również zastosować różne \textbf{metody łączenia klastrów}. Wykorzystamy do tej metody dane \texttt{spam\_variables}. Bierzemy w tym celu 100 losowych obserwacji, ponieważ biorąc cały zbiór danych złożoność obliczeniowa byłaby za duża, a wyniki - mniej przejrzyste i ciężkie do odczytania.

\begin{itemize}

\item \textbf{Odległość najbliższego sąsiada (single linkage)}
$$d_{SL}(G,H) = \min_{i\in G, j\in H}d_{ij}.$$
<<anal_skup_AGNES_single, echo=F, eval=T, fig.width=5, fig.height= 3,fig.pos="H", fig.align="center", out.extra="", fig.cap= "Single linkage",warning=F>>=
# Single linkage
spam_variables_hierarchic <- spam_variables
spam_variables_hierarchic <- scale(spam_variables_hierarchic)
spam_variables_hierarchic <- spam_variables_hierarchic[sample(nrow(spam_variables_hierarchic), 100), ]#losowo dobierane obserwacje

agnes.single <- agnes(spam_variables_hierarchic, method = "single", metric="euclid", stand=TRUE)
fviz_dend(agnes.single, cex = 0.5, main="AGNES - single linkage")
@

\item \textbf{Odległość najdalszego sąsiada (complete linkage)}
$$d_{CL}(G,H) = \max_{i\in G, j\in H}d_{ij}.$$

<<anal_skup_AGNES_complete, echo=F, eval=T, fig.width=5, fig.height= 3,fig.pos="H", fig.align="center", out.extra="", fig.cap= "Complete linkage",warning=F>>=
# Complete linkage
agnes.complete <- agnes(spam_variables_hierarchic, method = "complete", metric="euclid", stand=TRUE)
fviz_dend(agnes.complete, cex = 0.5, main="AGNES - complete linkage")
@

\item \textbf{Odległość średnia (average linkage)}
$$d_{AL}(G,H) = \frac{1}{|G|\cdot |H|}\sum_{i\in G, j\in H}d_{ij}.$$

<<anal_skup_AGNES_average, echo=F, eval=T, fig.width=5, fig.height= 3,fig.pos="H", fig.align="center", out.extra="", fig.cap= "Average linkage",warning=F>>=
# Average linkage
agnes.average <- agnes(spam_variables_hierarchic, method = "average", metric="euclid", stand=TRUE)
fviz_dend(agnes.average, cex = 0.5, main="AGNES - average linkage")

@
%może zastanowić się żeby zmienić na spam_variables_hierarchic[400,] albo więcej?
\end{itemize}
Pokażemy jeszcze dendogramy Average complete z podziałem na $k = 3$ i $k = 4$ klastry. Podział będzie widoczny poprzez zastosowanie kolorów na drzewie klastrów.
<<anal_skup_AGNES_k3, echo=F, eval=T, fig.width=6, fig.height= 4,fig.pos="H", fig.align="center", out.extra="", fig.cap= "Dendogram z podziałem na $k = 3$ skupień",warning=F>>=
# Dendrogram + podział na K skupień
fviz_dend(agnes.complete, k=3, cex=0.6, rect = TRUE, rect_fill=TRUE, main="AGNES - complete linkage (K=3)",color_labels_by_k = TRUE)
@

<<anal_skup_AGNES_k4, echo=F, eval=T, fig.width=6, fig.height= 4,fig.pos="H", fig.align="center", out.extra="", fig.cap= "Dendogram z podziałem na $k = 4$ skupień",warning=F>>=
fviz_dend(agnes.complete, k=4, cex=0.6, rect = TRUE, rect_fill=TRUE, main="AGNES - complete linkage (K=4)",color_labels_by_k = TRUE)
@


\item \textbf{Metody dzielące} w pierwszym kroku biorą wszystkie obiekty jako jedno duże skupienie, które następnie jest dzielone tak, aby otrzymać jednorodne grupy. Warto wspomnieć, że jedną z takich metod jest \textbf{DIANA}, lecz nie będziemy jej rozważać w naszej analizie.

\end{itemize}

\subsection{Metody grupowania gęstościowego}
Idea metody \textit{grupowania gęstościowego} odwołuje się do intuicyjnego sposobu odróżnienia skupień od "szumu" na podstawie analizy gęstości punktów z uwagi na to, że skupienia charakteryzują się zwykle dużą gęstością obiektów w danym obszarze. Metody te pozwalają na odkrywanie skupień o dowolnym kształcie, nie wymagają także określenia z góry liczby skupień \textit{K} oraz są efektywne w identyfikacji obserwacji odstających.

Przykładem klasycznego algorytmu grupowania opartego na gęstościach jest  \textbf{algorytm DBSCAN}. Wymaga on wyboru optymalnych wartości parametrów $\varepsilon$ oraz \textit{MinPts}. Jedną ze znaczących wad tej metody jest duża wrażliwość na wybór $\varepsilon$, zatem do ułatwienia tego wyboru możemy użyć metody, której ideą jest wyznaczenie dla każdego punktu jego odległości od \textit{k}-tego najbliższego sąsiada. Na wykresie \ref{fig:anal_skup_dbscan} będziemy szukać tzw. \textit{knee point}, czyli punktu, w którym następuje wyraźna zmiana nachylenia krzywej.

<<anal_skup_dbscan, echo=F, eval=T, fig.width=6, fig.height= 4,fig.pos="H", fig.align="center", out.extra="", fig.cap="Wybór optymalnych parametrów">>=
#to juz działa przynajmniej

# Wybór optymalnej wartości “epsilon”
dbscan::kNNdistplot(spam_variables, k=5)
title("DBSCAN: Wybór optymalnych parametrów")
grid()
abline(h = 9, lty = 2, col="red")
@

<<anal_skup_dbscan2, echo=F, eval=T, fig.width=6, fig.height= 4,fig.pos="H", fig.align="center", out.extra="", fig.cap="Wizualizacja dla metody DBSCAN epsilon = 9">>=
# DBSCAN
spam.dbscan1 <- dbscan::dbscan(spam_variables, 9, 5)

# wizualizacja
fviz_cluster(spam.dbscan1, spam_variables, geom = "point")
@

<<anal_skup_dbscan3, echo=F, eval=T, fig.width=6, fig.height= 3.5,fig.pos="H", fig.align="center", out.extra="", fig.cap="Wizualizacja dla metody DBSCAN epsilon = 7">>=

# inne parametry
# mniejsze eps
spam.dbscan2 <- dbscan::dbscan(spam_variables, 7, 5) 
fviz_cluster(spam.dbscan2, spam_variables, geom = "point")
@

<<anal_skup_dbscan4, echo=F, eval=T, fig.width=6, fig.height= 3.5,fig.pos="H", fig.align="center", out.extra="", fig.cap="Wizualizacja dla metody DBSCAN epsilon = 11">>=
# jeszcze mniejsze eps
spam.dbscan3 <- dbscan::dbscan(spam_variables, 11, 5)
fviz_cluster(spam.dbscan3, spam_variables, geom = "point")
@
Pierwszym nasuwającym się wnioskiem będzie brak efektywnego zastosowania metody \textit{DBSCAN} dla naszych danych, które na wykresie przedstawiają się jako jedna "zbita" grupa. Algorytm ten dla żadnej z wartości $\varepsilon$ (wykres \ref{fig:anal_skup_dbscan2}, \ref{fig:anal_skup_dbscan3} i \ref{fig:anal_skup_dbscan4}) nie jest skuteczny. Wynika to z tego, że zwizualizowany zbiór danych \textit{spambase} ma dużą gęstość punktów na całym zbiorze i metoda \textit{DBSCAN} nie potrafi zadecydować kiedy wyznaczyć następny klaster. Dodatkowo możemy zauważyć, że metoda ta określiła wiele punktów w środku zbioru jako obserwacje odstające (czarne punkty). %nie wiem czy dobrze tak na logike mysle, chyba jednak to nieprawda co napisałam bo siluette mowi co innego

\subsection{Ocena jakości analizy skupień}

Przeprowadzimy ocenę jakości analizy skupień za pomocą metody \textit{silhouette} dla każdego omówionego wcześniej algorytmu. Zanim jednak to zrobimy, opiszemy czym właściwie jest wspomniana metoda.
\\
\\
Wskaźnik \textit{silhouette} dla i-tego obiektu definiujemy jako
\[ s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}, \quad i = 1, \ldots, n \]
gdzie:
\begin{itemize}
\item \[ a(i) = \frac{1}{{|A|-1}} \sum_{j \in A, j \neq i} d_{ij} \]
\(a(i)\) - średnia odmienność i-tego obiektu od pozostałych obiektów z jego skupienia (\(A\) - skupienie, do którego należy i-ty obiekt).
\item \[ b(i) = \min_{C \neq A} d(i, C), \quad \text{gdzie } d(i, C) = \frac{1}{|C|} \sum_{j \in C} d_{ij} \]
\(b(i)\) - średnia odmienność między obiektem $i$ a najbliższym mu skupieniem.
\end{itemize}
Ma on następujące własności oraz interpretację:
\begin{itemize}
  \item Wskaźnik silhouette bierze pod uwagę dwie własności: zwartość klastrów i separację przestrzenną.
  \item $s(i) \in [-1, 1]$, im większa wartość, tym lepiej.
  \item Jeśli $s(i) \approx 1$, oznacza to, że obiekt $i$ jest poprawnie przyporządkowany.
  \item Jeśli $s(i) \approx 0$, oznacza to, że obiekt $i$ leży pomiędzy A i B.
  \item Jeśli $s(i) \approx -1$, oznacza to, że obiekt $i$ jest błędnie przyporządkowany.
  \item Aby ocenić jakość podziału na skupienia, zazwyczaj wyznacza się średnią wartość silhouette dla poszczególnych klastrów lub dla całej partycji.
\end{itemize}
Krótko mówiąc, podejście oparte na wskaźniku \textit{silhouette} mierzy jakość klastrów. Oznacza to, że określa, jak dobrze każdy obiekt leży w swoim klastrze. Optymalna liczba skupień \texttt{k} to taka, która maksymalizuje średnią wartość \textit{silhouette} w zakresie możliwych wartości dla \texttt{k}.

Możemy użyć funkcji \texttt{silhouette} z pakietu \texttt{cluster}, aby obliczyć średnią wartość wskaźnika silhouette. Średnia wartość wskaźnika \textit{silhouette} dla metody:
\begin{itemize}

\item \textbf{K-means}
<<ocena_silhouette_kmeans, echo=F, eval=T>>=
silhouette_score_kmeans <- silhouette(kmeans.k3$cluster, dist(spam_variables))
paste("Dla k = 3",summary(silhouette_score_kmeans)$avg.width)

silhouette_score_kmeans4 <- silhouette(kmeans.k4$cluster, dist(spam_variables))
paste("Dla k = 4",summary(silhouette_score_kmeans4)$avg.width)

silhouette_score_kmeans6 <- silhouette(kmeans.k6$cluster, dist(spam_variables))
paste("Dla k = 6", summary(silhouette_score_kmeans6)$avg.width)
@

\item \textbf{CLARA}

<<ocena_silhouette_clara, echo=F, eval=T>>=
silhouette_score_CLARA <- silhouette(part.CLARA$cluster, dist(spam_variables))
paste(summary(silhouette_score_CLARA)$avg.width)
@

\item \textbf{Fuzzy c-means}

<<ocena_silhouette_fuzzy, echo=F, eval=T>>=
silhouette_score_fuzzy <- silhouette(partycja.fcm$cluster, dist(spam_variables))
paste(summary(silhouette_score_fuzzy)$avg.width)
#tutaj mozna wziac kod od M.T. któremu to działa ale nie rozumiem go ani troche
@

\item \textbf{AGNES}

<<ocena_silhouette_AGNES_assign, echo=F, eval=T>>=
clust_assign_s <- cutree(agnes.single, k = 3)
clust_assign_c <- cutree(agnes.complete, k = 3)
clust_assign_a <- cutree(agnes.average, k = 3)
@

<<ocena_silhouette_AGNES, echo=F, eval=T>>=
silhouette_score_agnes_s <- silhouette(clust_assign_s, dist(spam_variables_hierarchic))
paste("Dla single linkage:",summary(silhouette_score_agnes_s)$avg.width)

silhouette_score_agnes_c <- silhouette(clust_assign_c, dist(spam_variables_hierarchic))
paste("Dla complete linkage:",summary(silhouette_score_agnes_c)$avg.width)

silhouette_score_agnes_a <- silhouette(clust_assign_a, dist(spam_variables_hierarchic))
paste("Dla average linkage:", summary(silhouette_score_agnes_a)$avg.width)
@

\item \textbf{DBSCAN}

<<ocena_silhouette_dbscan, echo=F, eval=T>>=
silhouette_score_dbscan1 <- silhouette(spam.dbscan1$cluster, dist(spam_variables))
paste("Dla eps = 9:",summary(silhouette_score_dbscan1)$avg.width)

silhouette_score_dbscan2 <- silhouette(spam.dbscan2$cluster, dist(spam_variables))
paste("Dla eps = 7:",summary(silhouette_score_dbscan2)$avg.width)

silhouette_score_dbscan3 <- silhouette(spam.dbscan3$cluster, dist(spam_variables))
paste("Dla  eps = 11:", summary(silhouette_score_dbscan3)$avg.width)
@

Powyższe wyniki świadczą o tym, że najwyższe średnie wartości wskaźnika \textit{silhouette} osiągane są przy stosowaniu metody \textbf{DBSCAN}, a najgorsze dla metody \textit{CLARA}.

\end{itemize}

\section{Redukcja wymiaru}
Celem redukcji wymiaru mogą być takie aspekty jak: eliminacja zbędnej lub nadmiarowej informacji, znalezienie popdprzestrzeni o mniejszym wymiarze, w których koncentruje się większość obserwacji, ekstrakcja cech, wizualizacja oraz identyfikacja struktury zależności w danych. Podejściem, którym się zajmiemy \textit{feature extraction}, gdzie konstruujemy nowe cechy na podstawie oryginalnych, a wtedy możemy odwzorować dane do nowej przestrzeni. Nowe cechy są odpowiednimi kombinacjami liniowymi oryginalnych. Przykładem takiego działania, który zastosujemy będzie \textbf{metoda PCA (Principal Component Analysis)}. Ta metoda redukcji wymiaru jest przykładem uczenia nienadzorowanego, w którym to nie wykorzystujemy informacji nt. przynależności obiektów do klas, a jego celem jest wykrycie wewnętrznej struktury danych.

Redukcja wymiaru na bazie \textit{PCA} polega na wyborze kilku pierwszych składowych głównych, które wyjaśniają frakcję całkowitej zmienności danych. Rozważamy kombinacje liniowe oryginalnych zmiennych
$$a_{1}X_{1} + a_{2}X_{2} + \ldots + a_{p}X_{p} = \underline{a}^{T}\underline{X},$$ gdzie $||\underline{a}||^{2} = a^{T}a = 1.$ Pierwsza składowa główna ($PC_{1}$) przedstawia się wzorem $$\text{Var}(\underline{a}^T_{1} X) = \max_{\underline{a} \in \mathbb{R}^p, \lVert \underline{a} \rVert = 1}\text{Var}(\underline{a}^T \underline{X}).$$ Druga składowa główna ($PC_{2}$) będzie wynosić $$\text{Var}(\underline{a}^T_{2} X) = \max_{\underline{a} \in \mathbb{R}^p, \lVert \underline{a} \rVert = 1}\text{Var}(\underline{a}^T \underline{X}),$$ $$\text{Cov}(\underline{a}^T_{1} \underline{X}, \underline{a}^T_{2} \underline{X}) = 0.$$ Następnie składowe będziemy obliczać według tego schematu i otrzymamy \textit{p} składowych głównych $PC_{1}, PC_{2}, \ldots, PC_{p}$. W naszym przypadku otrzymamy \textit{p} = 51 składowych głównych.

<<redukcja_wymiaru_pca1, echo=F, eval=T>>=
### Przykład 2: Analiza  dla większej liczby cech
data.after.pca <- prcomp(spam_variables, retx=T, center=T, scale.=T) 

#summary(data.after.pca)
@

<<redukcja_wymiaru__pca_var, echo=F, eval=T, fig.width=5.5, fig.height= 3.5,fig.pos="H", fig.align="center", out.extra="", fig.cap="Wariancja odpowiadająca poszczególnym składowym głównym">>=
### Wariancja wyjaśniona przez kolejne składowe główne
variance <- (data.after.pca$sdev^2)/sum(data.after.pca$sdev^2)
cumulative.variance <- cumsum(variance)
barplot(variance,col="lawngreen",main = "Wariancja")
grid(ny = 10)
@


<<redukcja_wymiaru_pca_plot, echo=F, eval=T, fig.width=5.5, fig.height= 3.5,fig.pos="H", fig.align="center", out.extra="", fig.cap="Wizualizacja w nowej przestrzeni (PC1, PC2)">>=
plot(data.after.pca$x[,1], data.after.pca$x[,2], pch=16,xlab="PC1",ylab="PC2")
@

Przydatnym narzędziem graficznym wykorzystywanym do znalezienia optymalnej liczby składowych jest wykres osypiskowy (ang. scree plot). Ustalamy frakcję wyjaśnionej wariancji na $80\% $ i wybieramy tę wartość dla której wykres osypiskowy (rysunek \ref{fig:redukcja_wymiaru_pca_scree}) po raz pierwszy zbliżony jest do linii poziomej. %i napisać która to wartość

<<redukcja_wymiaru_pca_var_sum, echo=F, eval=T, fig.width=5.5, fig.height= 3.5,fig.pos="H", fig.align="center", out.extra="", fig.cap="Wariancja skumulowana odpowiadająca poszczególnym składowym głównym">>=
barplot(cumulative.variance,col="aquamarine",main = "Skumulowana wariancja")
grid(ny = 10)
abline(h = 0.8, lty = 2, col="red")
text(x=1, y=0.85, '80%', col="red")
@

<<redukcja_wymiaru_pca_scree, echo=F, eval=T, fig.width=5.5, fig.height=3.5,fig.pos="H", fig.align="center", out.extra="", fig.cap="Wykres osypiskowy (ang. scree plot)">>=
#par(mfrow = c(1,2))
pca.results <- PCA(X=spam_variables, scale.unit = TRUE, graph = FALSE)
#par(mfrow = c(1,2))
 fviz_eig(pca.results, addlabels = TRUE, ncp = 20)
 
#chce jeszcze drugi wykres tutaj ktory bedzie przedstawial skumulowana sume i wtedy daj abline na poziomie 0.8 lub 0.9 i bedzie widac ile skladowych wziac
#mozna dodac drugi sposob ze wskazikiem ktory ma byc wiekszy niz 1
#dlaczego ten screee plot pokazuje tylko 10 kolumienek?
@

Drugim sposobem wyboru liczby składowych głównych jest pominięcie tych składowych, których wartości własne są mniejsze od wartości średniej $\overline{\lambda} = \frac{1}{p}\sum^{p}_{i=1}\lambda_{i}$.

<<redukcja_wymiaru_pca3, echo=FALSE, eval=TRUE, results='asis'>>=
eigenvalues <- get_eigenvalue(pca.results)
eigenvalues <- xtable(eigenvalues, 
               digits = 4, 
               row.names = TRUE, 
               caption = "Porównanie wartości zmiennej dla wszystkich składowych.", 
               label = "tab:part2")
colnames(eigenvalues) <- c("eigenvalue $\\lambda$","variance percent \\%","cumulative variance percent \\%")
align(eigenvalues) <- "c|c|c|c"
print(eigenvalues, type = "latex", table.placement = "H",sanitize.text.function=function(x){x}, scalebox = 0.75)

@

Suma wartości własnych jest równa całkowitej zmienności, czyli w naszym przypadku będzie to 51. Wartość \texttt{eigenvalue}$ > 1$ oznacza, że określonej składowej głównej odpowiada większa wariancja niż wariancja pojedynczej zmiennej. Wybieramy więc tylko te składowe, dla których parametr \texttt{eigenvalue} $> 1$.

<<redukcja_wymiaru_pca4, echo=F, eval=T, fig.width=8, fig.height=2,fig.pos="H", fig.align="center", out.extra="", fig.cap="Wykres korelacji zmiennych i składowych głównych",warning=FALSE>>=
variables <- get_pca_var(pca.results)
# loadings (ładunki)
loadings <- sweep(pca.results$var$coord,2,sqrt(pca.results$eig[1:ncol(pca.results$var$coord),1]),FUN="/")
variables_cor <- t(variables$cor)
# korelacje zmiennych z poszczególnymi składowymi głównymi
corrplot(variables_cor,number.cex = 0.4, tl.cex = 0.4)

@

<<redukcja_wymiaru_pca5, echo=F, eval=T, fig.width=6, fig.height=4,fig.pos="H", fig.align="center", out.extra="", fig.cap="Wykres korelacji zmiennych i składowych głównych",warning=FALSE>>=

# wykres zmiennych (wpływ poszczególnych zmiennych na wartości dwóch pierwszych składowych głównych)
fviz_pca_var(pca.results, col.var="black",labelsize = 4, repel=TRUE)
@

Analizę wykresu \ref{fig:redukcja_wymiaru_pca5} można zapisać w dwóch poniższych punktach:
\begin{itemize}

\item Jeżeli wektory odpowiadające zmiennym mają przeciwne zwroty, zmienne są ujemnie skorelowane, a jeżeli zwroty są bliskie – zmienne są dodatnio
skorelowane.
\item Jeżeli zwroty są prostopadłe, zmienne są nieskorelowane.

\end{itemize}

\newpage
Przedstawimy wkład zmiennych do dwóch skłądowych głównych PC1 i PC2.
<<redukcja_wymiaru_pca7, echo=F, eval=T, fig.width=6, fig.height=4,fig.pos="H", fig.align="center", out.extra="", fig.cap="Wkład zmiennych do PC1",warning=FALSE>>=
fviz_contrib(pca.results, choice="var", axes=1)
@

<<redukcja_wymiaru_pca8, echo=F, eval=T, fig.width=6, fig.height=4,fig.pos="H", fig.align="center", out.extra="", fig.cap="Wkład zmiennych do PC2",warning=FALSE>>=
fviz_contrib(pca.results, choice="var", axes=2)
@

<<redukcja_wymiaru_pca9, echo=F, eval=T, fig.width=6, fig.height=4,fig.pos="H", fig.align="center", out.extra="", fig.cap="wykres zmiennych wraz z kolorami odpowiadającymi ważności/wkładowi poszczególnych zmiennych",warning=FALSE>>=
fviz_pca_var(pca.results, col.var ="contrib", gradient.cols=c("#00AFBB","#E7B800","#FC4E07"))
@

<<redukcja_wymiaru_pca10, echo=F, eval=T, fig.width=6, fig.height=4,fig.pos="H", fig.align="center", out.extra="", fig.cap="">>=
### Wykresy dla przypadków (obserwacji) 

# Wykres rozrzutu w przestrzeni (PC1,PC2)
fviz_pca_ind(pca.results, repel=TRUE)

# Dodanie informacji nt. przynależności do grup (zmienna Competition)
fviz_pca_ind(pca.results, col.ind = CleanSpam$type,repel=TRUE)

# Kolory wyznaczone przez zmienną ciągłą (zmienna Points)
fviz_pca_ind(pca.results, repel=TRUE, col.ind = CleanSpam$type)
#gradient.cols=c("#00AFBB","#E7B800","#FC4E07"))

# Zastosowanie metody k-means
dane2d=pca.results$ind$coord[,1:2]
kmeans = kmeans(dane2d, nstart=10, centers = 4)
fviz_cluster(kmeans, dane2d)

##########################################################################################
### Dwuwykresy (ang. biplot)

# domyślne parametry
fviz_pca_biplot(pca.results)

# usuwamy nazwy przypadków
fviz_pca_biplot(pca.results, label="var")

# dodajemy kolory odpowiadające ustalonym grupom/klasom
fviz_pca_biplot(pca.results, label="var", col.ind = decathlon2$Competition)

# dodajemy elipsy koncentracji
fviz_pca_biplot(pca.results, label="var", col.ind = CleanSpam$type, addEllipses = TRUE)

@
\end{document}